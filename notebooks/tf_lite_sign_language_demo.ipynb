{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-05-18T16:49:19.121697Z",
     "iopub.status.busy": "2023-05-18T16:49:19.121327Z",
     "iopub.status.idle": "2023-05-18T16:49:19.128117Z",
     "shell.execute_reply": "2023-05-18T16:49:19.127005Z",
     "shell.execute_reply.started": "2023-05-18T16:49:19.121669Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T16:49:22.191894Z",
     "iopub.status.busy": "2023-05-18T16:49:22.190981Z",
     "iopub.status.idle": "2023-05-18T16:49:22.198552Z",
     "shell.execute_reply": "2023-05-18T16:49:22.197488Z",
     "shell.execute_reply.started": "2023-05-18T16:49:22.191857Z"
    }
   },
   "outputs": [],
   "source": [
    "my_seed = 42\n",
    "split_proportion = 0.1\n",
    "batch_size = 4\n",
    "number_epochs = 8\n",
    "lr = 3e-5\n",
    "\n",
    "figure_count = 0\n",
    "figure_dir = os.path.join(\"..\", \"assets\")\n",
    "\n",
    "train_new = False\n",
    "\n",
    "print(f\"Number of GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download the dataset if you are working on colab\n",
    "#! kaggle datasets download -d ardamavi/27-class-sign-language-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T16:49:23.233265Z",
     "iopub.status.busy": "2023-05-18T16:49:23.232881Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dir = os.path.join(\"..\", \"input\", \"27-class-sign-language-dataset\")\n",
    "\n",
    "x_filename = os.path.join(input_dir, \"X.npy\")\n",
    "y_filename = os.path.join(input_dir, \"Y.npy\")\n",
    "\n",
    "x = np.load(x_filename)\n",
    "y = np.load(y_filename)\n",
    "\n",
    "# shuffle and split the data\n",
    "split_number = int(split_proportion * x.shape[0])\n",
    "\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(x)\n",
    "val_x = tf.convert_to_tensor(x[:split_number])\n",
    "test_x = tf.convert_to_tensor(x[split_number:2*split_number])\n",
    "train_x = tf.convert_to_tensor(x[2*split_number:])\n",
    "\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(y)\n",
    "val_y_labels = tf.convert_to_tensor(y[:split_number])\n",
    "test_y_labels = tf.convert_to_tensor(y[split_number:2*split_number])\n",
    "train_y_labels = tf.convert_to_tensor(y[2*split_number:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(input_dir, \"mini_X.npy\"), x[:4096])\n",
    "np.save(os.path.join(input_dir, \"mini_y.npy\"), y[:4096])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize images with labels\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(8,8))\n",
    "for count, x_index in enumerate(np.random.randint(0, train_x.shape[0], size=(9,))):\n",
    "\n",
    "    cx = count // 3\n",
    "    cy = count % 3\n",
    "    ax[cx,cy].imshow(train_x[x_index])\n",
    "    ax[cx,cy].set_title(f\"label: {train_y_labels[x_index]}\")\n",
    "    ax[cx,cy].set_yticklabels(\"\")\n",
    "    ax[cx,cy].set_xticklabels(\"\")\n",
    "    \n",
    "plt.savefig(os.path.join(figure_dir, f\"figure_{figure_count}.png\"))\n",
    "figure_count += 1\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for number, label in enumerate(np.unique(train_y_labels)):\n",
    "    label_dict[number] = label\n",
    "    \n",
    "print(label_dict, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_label_dict = {}\n",
    "for key in label_dict.keys():\n",
    "    reverse_label_dict[label_dict[key]] = key\n",
    "    \n",
    "print(reverse_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_y = np.zeros_like(train_y_labels)\n",
    "np_val_y = np.zeros_like(val_y_labels)\n",
    "np_test_y = np.zeros_like(test_y_labels)\n",
    "\n",
    "for ii in range(np_train_y.shape[0]):\n",
    "    np_train_y[ii] = reverse_label_dict[train_y_labels[ii].numpy()[0]]\n",
    "    \n",
    "for ii in range(np_val_y.shape[0]):\n",
    "    np_val_y[ii] = reverse_label_dict[val_y_labels[ii].numpy()[0]]\n",
    "    \n",
    "for ii in range(np_test_y.shape[0]):\n",
    "    np_test_y[ii] = reverse_label_dict[test_y_labels[ii].numpy()[0]]\n",
    "    \n",
    "train_y = tf.convert_to_tensor(np_train_y.reshape(-1), dtype=tf.int32)\n",
    "val_y = tf.convert_to_tensor(np_val_y.reshape(-1), dtype=tf.int32)\n",
    "test_y = tf.convert_to_tensor(np_test_y.reshape(-1), dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T16:38:29.689940Z",
     "iopub.status.busy": "2023-05-18T16:38:29.689587Z",
     "iopub.status.idle": "2023-05-18T16:38:30.912689Z",
     "shell.execute_reply": "2023-05-18T16:38:30.911878Z",
     "shell.execute_reply.started": "2023-05-18T16:38:29.689906Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize images with labels\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize=(8,8))\n",
    "for count, x_index in enumerate(np.random.randint(0, val_x.shape[0], size=(9,))):\n",
    "\n",
    "    cx = count // 3\n",
    "    cy = count % 3\n",
    "    idx = val_y[x_index]\n",
    "    ax[cx,cy].imshow(val_x[x_index])\n",
    "    ax[cx,cy].set_title(f\"label index: \\n {idx} = {label_dict[idx.numpy()]}\")\n",
    "    ax[cx,cy].set_yticklabels(\"\")\n",
    "    ax[cx,cy].set_xticklabels(\"\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(figure_dir, f\"figure_{figure_count}.png\"))\n",
    "figure_count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T16:38:33.058027Z",
     "iopub.status.busy": "2023-05-18T16:38:33.057642Z",
     "iopub.status.idle": "2023-05-18T16:38:35.059633Z",
     "shell.execute_reply": "2023-05-18T16:38:35.058629Z",
     "shell.execute_reply.started": "2023-05-18T16:38:33.057974Z"
    }
   },
   "outputs": [],
   "source": [
    "number_classes = len(label_dict.keys())\n",
    "\n",
    "extractor = tf.keras.applications.MobileNet(\\\n",
    "    input_shape=train_x.shape[1:], include_top=False,weights=\"imagenet\")\n",
    "    \n",
    "\n",
    "extractor.trainable = True\n",
    "\n",
    "\n",
    "model = Sequential([extractor, \\\n",
    "        tf.keras.layers.Flatten(),\\\n",
    "        tf.keras.layers.Dropout(0.25),\\\n",
    "        Dense(32, activation=\"relu\"),\\\n",
    "        Dense(32, activation=\"relu\"),\\\n",
    "        Dense(number_classes, activation=\"softmax\")])\n",
    "\n",
    "#model.build([None, 128, 128, 3])\n",
    "\n",
    "\n",
    "_ = model(train_x[0:1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T16:38:35.094883Z",
     "iopub.status.busy": "2023-05-18T16:38:35.093946Z",
     "iopub.status.idle": "2023-05-18T16:38:35.153681Z",
     "shell.execute_reply": "2023-05-18T16:38:35.152702Z",
     "shell.execute_reply.started": "2023-05-18T16:38:35.094850Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\\\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics = ['accuracy']\n",
    "             )\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scheduler(my_lr):\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch <= 1:\n",
    "            return my_lr / 10.\n",
    "        elif epoch == 2:\n",
    "            return my_lr * 10.\n",
    "        else:\n",
    "            return lr * 0.9 \n",
    "    \n",
    "    return scheduler\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\\\n",
    "    log_dir=\"logs\", \\\n",
    "    write_graph=True, \\\n",
    "    update_freq='epoch', \\\n",
    ")\n",
    "\n",
    "scheduler = make_scheduler(lr)\n",
    "lr_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_dir = os.path.join(\"..\", \"models\", \"mobilenet_sign\")\n",
    "tf_lite_model_filename = os.path.join(\"..\", \"models\", \"tflite_mobilenet.tflite\")\n",
    "\n",
    "if train_new:\n",
    "    history = model.fit(x=train_x, y=train_y, validation_data=(val_x, val_y), \\\n",
    "        batch_size=batch_size, epochs=number_epochs, \\\n",
    "        callbacks=[tensorboard_callback, lr_scheduler_callback])\n",
    "    \n",
    "    model.save(save_model_dir)\n",
    "else:\n",
    "    model = tf.keras.models.load_model(save_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (1):\n",
    "    # save the keras model in directory save_model_dir \n",
    "    # then convert the SavedModel directory to a TF Lite model\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(save_model_dir)\n",
    "    tf_lite_mobilenet = converter.convert()\n",
    "else:\n",
    "    # alternatively, convert from the keras model without saving first\n",
    "    convert = tf.lit.TFLiteConvert.from_keras_model(model)\n",
    "    tf_lite_mobilenet = converter.convert()  \n",
    "\n",
    "# Save the model.\n",
    "with open(tf_lite_model_filename, \"wb\") as f:\n",
    "    f.write(tf_lite_mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tf_lite_model_filename)\n",
    "my_signature = interpreter.get_signature_runner()\n",
    "\n",
    "for jj in range(4):\n",
    "    \n",
    "    my_index = np.random.randint(0, val_x.shape[0])\n",
    "    \n",
    "    my_batch = val_x[my_index:my_index+1]\n",
    "    \n",
    "    t0 = time.time()\n",
    "    output_data = my_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "    t1 = time.time()\n",
    "    full_output_data = model(my_batch)\n",
    "    t2 = time.time()\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(8,5))\n",
    "    \n",
    "    ax[0].imshow(my_batch[0].numpy())\n",
    "    ax[0].set_title(f\"TF Lite pred. ({(t1-t0):.1e} s) \\n label: {label_dict[output_data.argmax()]}\")\n",
    "    ax[1].imshow(my_batch[0].numpy())\n",
    "    ax[1].set_title(f\"TF keras pred. ({(t2-t1):.1e} s)\\n label: {label_dict[full_output_data.numpy().argmax()]}\")\n",
    "    \n",
    "    ax[0].set_yticklabels(\"\")\n",
    "    ax[0].set_xticklabels(\"\")\n",
    "    fig.suptitle(f\"True label: {label_dict[val_y[my_index].numpy()]}\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(figure_dir, f\"figure_{figure_count}.png\"))\n",
    "    figure_count += 1\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset():\n",
    "    \n",
    "    for data in tf.data.Dataset.from_tensor_slices((val_x)).batch(1).take(100):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "        \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(save_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "tf_lite_mobilenet_quantized = converter.convert()\n",
    "tf_lite_quant_filename = os.path.join(\"..\", \"models\", \"tflite_mobilenet_quant.tflite\")\n",
    "\n",
    "# Save the model.\n",
    "with open(tf_lite_quant_filename, \"wb\") as f:\n",
    "    f.write(tf_lite_mobilenet_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quant_interpreter = tf.lite.Interpreter(model_path=tf_lite_quant_filename)\n",
    "quant_signature = quant_interpreter.get_signature_runner()\n",
    "\n",
    "for jj in range(4):\n",
    "    \n",
    "    my_index = np.random.randint(0, val_x.shape[0])\n",
    "    \n",
    "    my_batch = val_x[my_index:my_index+1]\n",
    "    \n",
    "    t0 = time.time()\n",
    "    quant_output_data = quant_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "    t1 = time.time()\n",
    "    full_output_data = model(my_batch)\n",
    "    t2 = time.time()\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, figsize=(8,5))\n",
    "    \n",
    "    ax[0].imshow(my_batch[0].numpy())\n",
    "    ax[0].set_title(f\"Quant. Lite pred. ({(t1-t0):.1e} s) \\n label: {label_dict[quant_output_data.argmax()]}\")\n",
    "    ax[1].imshow(my_batch[0].numpy())\n",
    "    ax[1].set_title(f\"TF keras pred. ({(t2-t1):.1e} s)\\n label: {label_dict[full_output_data.numpy().argmax()]}\")\n",
    "    \n",
    "    ax[0].set_yticklabels(\"\")\n",
    "    ax[0].set_xticklabels(\"\")\n",
    "    fig.suptitle(f\"True label: {label_dict[val_y[my_index].numpy()]}\", fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(figure_dir, \"figure_{figure_count}.png\"))\n",
    "    figure_count += 1\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    t3 = time.time()\n",
    "\n",
    "    for my_index in range(val_x.shape[0]):\n",
    "\n",
    "        my_batch = val_x[my_index:my_index+1]\n",
    "        full_output_data = model(my_batch)\n",
    "\n",
    "    t4 = time.time()\n",
    "    print(\"finished with full keras model\")\n",
    "\n",
    "    for my_index in range(val_x.shape[0]):\n",
    "\n",
    "        my_batch = val_x[my_index:my_index+1]\n",
    "        output_data = my_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "\n",
    "    t5 = time.time()\n",
    "    print(\"finished with TF Lite model\")\n",
    "\n",
    "    for my_index in range(val_x.shape[0]):\n",
    "\n",
    "        my_batch = val_x[my_index:my_index+1]\n",
    "        quant_output_data = quant_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "\n",
    "    t6 = time.time()\n",
    "    print(\"finished with quantized TF Lite model\")\n",
    "\n",
    "msg = f\"time elapsed {val_x.shape[0]} samples \\n\\t keras: {t4-t3:.2f} s \\n\\t TF Lite: {t5-t4:.2f} s\"\n",
    "msg += f\" \\n\\t Quantized TF Lite: {t6-t5:.3f} s\"\n",
    "print(msg)\n",
    "\"\"\"\n",
    "finished with full keras model\n",
    "finished with TF Lite model\n",
    "finished with quantized TF Lite model\n",
    "time elapsed 2280 samples \n",
    "\t keras: 77.63 s \n",
    "\t TF Lite: 13.34 s \n",
    "\t Quantized TF Lite: 21.718 s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_keras = 0\n",
    "correct_lite = 0\n",
    "correct_quant = 0\n",
    "total_samples = val_x.shape[0]\n",
    "\n",
    "for my_index in range(val_x.shape[0]):\n",
    "\n",
    "    my_batch = val_x[my_index:my_index+1]\n",
    "    \n",
    "    full_output_data = model(my_batch)\n",
    "    output_data = my_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "    \n",
    "    quant_output_data = quant_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "                        \n",
    "    \n",
    "    true_label = val_y[my_index].numpy()\n",
    "    \n",
    "    correct_keras += 1.0 * (full_output_data.numpy().argmax() == true_label)\n",
    "    correct_lite += 1.0 * (output_data.argmax() == true_label)\n",
    "    correct_quant += 1.0 * (quant_output_data.argmax() == true_label)\n",
    "    \n",
    "accuracy_quant = correct_quant / total_samples\n",
    "accuracy_lite = correct_lite / total_samples\n",
    "accuracy_keras = correct_keras / total_samples\n",
    "\n",
    "msg = f\"Validation accuracies \"\n",
    "msg += f\"\\n\\t keras {accuracy_keras:.4f}\"\n",
    "msg += f\"\\n\\t TF Lite {accuracy_lite:.4f}\"\n",
    "msg += f\"\\n\\t Quantized TF Lite {accuracy_quant:.4f}\" \n",
    "\n",
    "print(msg)\n",
    "\"\"\"\n",
    "accuracies \n",
    "\t keras 0.9851\n",
    "\t TF Lite 0.9851\n",
    "\t Quantized TF Lite 0.9855\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_keras = 0\n",
    "correct_lite = 0\n",
    "correct_quant = 0\n",
    "total_samples = test_x.shape[0]\n",
    "\n",
    "for my_index in range(test_x.shape[0]):\n",
    "\n",
    "    my_batch = test_x[my_index:my_index+1]\n",
    "    \n",
    "    full_output_data = model(my_batch)\n",
    "    output_data = my_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "    \n",
    "    quant_output_data = quant_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "                        \n",
    "    \n",
    "    true_label = test_y[my_index].numpy()\n",
    "    \n",
    "    correct_keras += 1.0 * (full_output_data.numpy().argmax() == true_label)\n",
    "    correct_lite += 1.0 * (output_data.argmax() == true_label)\n",
    "    correct_quant += 1.0 * (quant_output_data.argmax() == true_label)\n",
    "    \n",
    "accuracy_quant = correct_quant / total_samples\n",
    "accuracy_lite = correct_lite / total_samples\n",
    "accuracy_keras = correct_keras / total_samples\n",
    "\n",
    "msg = f\"Test accuracies \"\n",
    "msg += f\"\\n\\t keras {accuracy_keras:.4f}\"\n",
    "msg += f\"\\n\\t TF Lite {accuracy_lite:.4f}\"\n",
    "msg += f\"\\n\\t Quantized TF Lite {accuracy_quant:.4f}\" \n",
    "\n",
    "print(msg)\n",
    "\"\"\"\n",
    "Test accuracies \n",
    "\t keras 0.9882\n",
    "\t TF Lite 0.9882\n",
    "\t Quantized TF Lite 0.9877\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_test = True\n",
    "\n",
    "if pi_test:    \n",
    "    \n",
    "    # load dataset (a validation split of)\n",
    "    # CC-BY-NC-SA Mavi and Dikle and Turkey Ankara Ayrancı Anadolu High School\n",
    "    # https://www.kaggle.com/datasets/ardamavi/27-class-sign-language-dataset\n",
    "    # associated arXiv manuscript 2203.03859\n",
    "    \n",
    "    dim_x, dim_y = 128, 128\n",
    "    data_dir = os.path.join(\"..\", \"input\", \"27-class-sign-language-dataset\")\n",
    "    \n",
    "    x_filename = os.path.join(data_dir, \"val_x.npy\")\n",
    "    y_filename = os.path.join(data_dir, \"val_y.npy\")\n",
    "    \n",
    "    pi_x = np.load(x_filename)\n",
    "    pi_y = np.load(y_filename)\n",
    "    \n",
    "    # load the Keras, TF Lite, and TF Lite quantized model\n",
    "    \n",
    "    save_model_dir = os.path.join(\"..\", \"models\", \"mobilenet_sign\")\n",
    "    tf_lite_model_filename = os.path.join(\"..\", \"models\", \"tflite_mobilenet.tflite\")\n",
    "    tf_lite_quant_filename = os.path.join(\"..\", \"models\", \"tflite_mobilenet_quant.tflite\")\n",
    "  \n",
    "    model = tf.keras.models.load_model(save_model_dir)\n",
    "\n",
    "    interpreter = tf.lite.Interpreter(model_path=tf_lite_model_filename)\n",
    "    quant_interpreter = tf.lite.Interpreter(model_path=tf_lite_quant_filename)\n",
    "    \n",
    "    tf_lite_signature = interpreter.get_signature_runner()\n",
    "    quant_signature = quant_interpreter.get_signature_runner()\n",
    "    \n",
    "    correct_keras = 0\n",
    "    correct_lite = 0\n",
    "    correct_quant = 0\n",
    "    \n",
    "    t_quant = 0.\n",
    "    t_lite = 0.\n",
    "    t_keras = 0.\n",
    "    \n",
    "    for my_index in range(pi_x.shape[0]):\n",
    "\n",
    "        my_batch = pi_x[my_index:my_index+1]\n",
    "\n",
    "        t0_tf_lite = time.time()\n",
    "        output_data = my_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "        t1_tf_lite = time.time()\n",
    "        \n",
    "        t0_keras = time.time()\n",
    "        full_output_data = model(my_batch)\n",
    "        t1_keras = time.time()\n",
    "        \n",
    "        t0_quant = time.time()\n",
    "        quant_output_data = quant_signature(**{\"mobilenet_1.00_128_input\": my_batch})[\"dense_2\"]\n",
    "        t1_quant = time.time()\n",
    "\n",
    "        true_label = pi_y[my_index]#.numpy()\n",
    "\n",
    "        correct_keras += 1.0 * (full_output_data.numpy().argmax() == true_label)\n",
    "        correct_lite += 1.0 * (output_data.argmax() == true_label)\n",
    "        correct_quant += 1.0 * (quant_output_data.argmax() == true_label)\n",
    "        \n",
    "        t_quant += t1_quant - t0_quant\n",
    "        t_keras += t1_keras - t0_keras\n",
    "        t_lite += t1_tf_lite - t0_tf_lite\n",
    "        \n",
    "\n",
    "\n",
    "    samples_seen = pi_x.shape[0]\n",
    "\n",
    "    quant_accuracy = correct_quant / samples_seen\n",
    "    keras_accuracy = correct_keras / samples_seen\n",
    "    lite_accuracy = correct_lite / samples_seen\n",
    "\n",
    "    avg_t_quant = t_quant / samples_seen\n",
    "    avg_t_keras = t_keras / samples_seen\n",
    "    avg_t_lite = t_lite / samples_seen\n",
    "\n",
    "    msg = f\"\\n quant \\n\\t avg. inference time = {avg_t_quant:.3e} \\n\\t accuracy {quant_accuracy}\"\n",
    "    msg += f\"\\n tf lite \\n\\t avg. inference time = {avg_t_lite:.3e} \\n\\t accuracy {lite_accuracy}\"\n",
    "    msg += f\"\\n keras \\n\\t avg. inference time = {avg_t_keras:.3e} \\n\\t accuracy {keras_accuracy}\"\n",
    "\n",
    "    print(msg)\n",
    "    \"\"\"\n",
    "    # on desktop:\n",
    "    quant \n",
    "        avg. inference time = 9.547e-03 \n",
    "        accuracy 0.9855263157894737\n",
    "    tf lite \n",
    "        avg. inference time = 5.612e-03 \n",
    "        accuracy 0.9850877192982456\n",
    "    keras \n",
    "        avg. inference time = 5.612e-03 \n",
    "        accuracy 0.9850877192982456\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
